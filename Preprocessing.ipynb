{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "embedding = KeyedVectors.load_word2vec_format(\"../Waste/GoogleNews-vectors-negative300.bin\", binary=True)  # C bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "    data = str(data)\n",
    "    # Replacing numbers with '#'\n",
    "    for i in [8, 7, 6, 5, 4, 3, 2, 1]:\n",
    "        temp = \"\".join(['#' for j in range(i)])\n",
    "        data = re.sub(f\"[0-9]{{{i}}}\", temp, data)\n",
    "    misspell_dict = {'colour':'color',\n",
    "                'centre':'center',\n",
    "                'didnt':'did not',\n",
    "                'doesnt':'does not',\n",
    "                'isnt':'is not',\n",
    "                'shouldnt':'should not',\n",
    "                \"He'll\":'He will',\n",
    "                \"She'll\":'She will',\n",
    "                \"i'm\": \"I am\",\n",
    "                \"e-mail\": \"email\",\n",
    "                'favourite':'favorite',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'labour':'labor',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'social medium',\n",
    "                'whatsapp': 'social medium',\n",
    "                'snapchat': 'social medium',\n",
    "                \"₹\": \" rupee \",\n",
    "                '&': \" & \",\n",
    "                '/': \" \",\n",
    "                '-': \" \",\n",
    "                }\n",
    "    for key, value in misspell_dict.items():\n",
    "        data = re.sub(key, value, data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(clean_text)\n",
    "df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../Dataset/preprocessed_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=10000, shuffle=True, stratify=df['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented to not accidentally override previously saved files unless you manually uncomment it\n",
    "\n",
    "# train_df.to_csv(\"Dataset/train_df.csv\")\n",
    "# test_df.to_csv(\"Dataset/test_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../Dataset/train_df.csv\")\n",
    "test_df = pd.read_csv(\"../Dataset/test_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_train = train_df['question1'].values\n",
    "q2_train = train_df['question2'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../Dataset/q1_train.npy', q1_train)\n",
    "np.save('../Dataset/q2_train.npy', q2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_test = test_df['question1'].values\n",
    "q2_test = test_df['question2'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../Dataset/q1_test.npy', q1_test)\n",
    "np.save('../Dataset/q2_test.npy', q2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['is_duplicate'].values\n",
    "y_test = test_df['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../Dataset/y_train.npy', y_train)\n",
    "np.save('../Dataset/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing train set\n",
    "q1_train_npy = np.empty_like(q1_train)\n",
    "q2_train_npy = np.empty_like(q2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing and creating Vocabulary list\n",
    "from collections import defaultdict\n",
    "# lambda: 0 makes the dictionary return value 0 when the key is not present in it but accessed\n",
    "vocab = defaultdict(lambda: 0)\n",
    "# '<PAD>' = 1 is used for padding the sequences\n",
    "vocab['<PAD>'] = 1\n",
    "# 0 is to represent words that are not in vocab and the index of word in voab_idx = vocab[word]\n",
    "vocab_idx = [0, '<PAD>']\n",
    "# These punctuations are not in word_embeddings. So we will not keep them in our train sequences\n",
    "punctuation = {'?', '!', '.', ',', '\"', \"'\", '(', ')', '-', '/', ':', ';', '<', '[', '\\\\', ']', '{', '|', '}', '“', '”', '’', ''}\n",
    "for i in range(len(q1_train)):\n",
    "    q1_train_npy[i] = [token.text for token in nlp(q1_train[i]) if token.text not in punctuation]\n",
    "    q2_train_npy[i] = [token.text for token in nlp(q2_train[i]) if token.text not in punctuation]\n",
    "    for word in q1_train_npy[i]+q2_train_npy[i]:\n",
    "        if word in stopwords and word not in embedding.vocab:\n",
    "            continue\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab) + 1\n",
    "            vocab_idx.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../Dataset/q1_train_tokenized.npy', q1_train_npy)\n",
    "np.save('../Dataset/q2_train_tokenized.npy', q2_train_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing test set\n",
    "q1_test_npy = np.empty_like(q1_test)\n",
    "q2_test_npy = np.empty_like(q2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These punctuations are not in word_embeddings. So we will not keep them in our test sequences\n",
    "punctuation = {'?', '!', '.', ',', '\"', \"'\", '(', ')', '-', '/', ':', ';', '<', '[', '\\\\', ']', '{', '|', '}', '“', '”', '’', ''}\n",
    "for i in range(len(q1_test)):\n",
    "    q1_test_npy[i] = [token.text for token in nlp(q1_test[i]) if token.text not in punctuation]\n",
    "    q2_test_npy[i] = [token.text for token in nlp(q2_test[i]) if token.text not in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../Dataset/q1_test_tokenized.npy', q1_test_npy)\n",
    "np.save('../Dataset/q2_test_tokenized.npy', q2_test_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the vocab_idx for future use\n",
    "np.save('../Dataset/vocab_idx.npy', vocab_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding vocab defaultdict to list format: Ex: vocab['Hello']=1; then vocab_list=[['Hello', 1]]\n",
    "vocab_list = []\n",
    "for key, value in vocab.items():\n",
    "    vocab_list.append((key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the above created vocab_list for future use\n",
    "np.save(\"../Dataset/vocab_list.npy\", vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the vocab-idx and creating unique vocab from the loaded file\n",
    "vocab_idx = np.load(\"../Dataset/vocab_idx.npy\", allow_pickle='True')\n",
    "\n",
    "# Recreating vocab defaultdict from vocab_idx file\n",
    "from collections import defaultdict\n",
    "vocab = defaultdict(lambda:0)\n",
    "for idx, word in enumerate(vocab_idx[1:]):\n",
    "    vocab[word] = idx+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the tokenized train sequences\n",
    "for i in range(len(q1_train_npy)):\n",
    "    q1_train_npy[i] = [vocab[word] for word in q1_train_npy[i]]\n",
    "    q2_train_npy[i] = [vocab[word] for word in q2_train_npy[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../Dataset/q1_train_encoded.npy', q1_train_npy)\n",
    "np.save('../Dataset/q2_train_encoded.npy', q2_train_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the tokenized test sequences\n",
    "for i in range(len(q1_test_npy)):\n",
    "    q1_test_npy[i] = [vocab[word] for word in q1_test_npy[i]]\n",
    "    q2_test_npy[i] = [vocab[word] for word in q2_test_npy[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('../Dataset/q1_test_encoded.npy', q1_test_npy)\n",
    "# np.save('../Dataset/q2_test_encoded.npy', q2_test_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can't directly save the vocab file of type defaultdict because of lambda\n",
    "# PicklingError: Can't pickle <function <lambda> at 0x00000223530A1CA0>: attribute lookup <lambda> on __main__ failed\n",
    "\n",
    "\n",
    "# np.save('Dataset/vocab.npy', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting embedding_matrix from the loaded pre-trained word embedding\n",
    "embed_dim = 300\n",
    "embedding_matrix = np.empty(shape=(len(vocab_idx), embed_dim))\n",
    "embedding_matrix[0] = [0 for i in range(embed_dim)] # Words which are not in vocab but in questions \n",
    "embedding_matrix[1] = [0 for i in range(embed_dim)] # <PAD>: To ignore padding\n",
    "for word in vocab_idx[2:]:\n",
    "    if word in embedding.vocab:\n",
    "        embedding_matrix[vocab[word]] = embedding[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(embedding_matrix)):\n",
    "    assert len(embedding_matrix[i]) == 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../Dataset/embedding_matrix.npy\", embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length is taken as the length of the largest question in our train set\n",
    "# with max_seq_len=60 it covers more than 99.985% questions within this range\n",
    "max_seq_len = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "q1_train_npy = pad_sequences(q1_train_npy, maxlen=max_seq_len, value=1)\n",
    "q2_train_npy = pad_sequences(q2_train_npy, maxlen=max_seq_len, value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../Dataset/q1_train_padded.npy', q1_train_npy)\n",
    "np.save('../Dataset/q2_train_padded.npy', q2_train_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_test_npy = pad_sequences(q1_test_npy, maxlen=max_seq_len, value=1)\n",
    "q2_test_npy = pad_sequences(q2_test_npy, maxlen=max_seq_len, value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../Dataset/q1_test_padded.npy', q1_test_npy)\n",
    "np.save('../Dataset/q2_test_padded.npy', q2_test_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_train_padded = np.load(\"../Dataset/q1_train_padded.npy\", allow_pickle='True')\n",
    "q2_train_padded = np.load(\"../Dataset/q2_train_padded.npy\", allow_pickle='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manual_features_npy = np.load(\"../manual_features/train_manual_features_npy.npy\", allow_pickle='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.load(\"../Dataset/y_train.npy\", allow_pickle='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_idx = np.load(\"../Dataset/vocab_idx.npy\", allow_pickle='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating vocab defaultdict from vocab_idx file\n",
    "from collections import defaultdict\n",
    "vocab = defaultdict(lambda:0)\n",
    "for idx, word in enumerate(vocab_idx[1:]):\n",
    "    vocab[word] = idx+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.load(\"../Dataset/embedding_matrix.npy\", allow_pickle='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=40000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, valid_idx in sss.split(q1_train_padded, y_train):\n",
    "    q1_train_final, q2_train_final = q1_train_padded[train_idx], q2_train_padded[train_idx]\n",
    "    q1_valid_final, q2_valid_final = q1_train_padded[valid_idx], q2_train_padded[valid_idx]\n",
    "    train_manual_features_final = train_manual_features_npy[train_idx]\n",
    "    valid_manual_features_final = train_manual_features_npy[valid_idx]\n",
    "    y_train_final, y_valid_final = y_train[train_idx], y_train[valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../Dataset/q1_train_final.npy\", q1_train_final)\n",
    "np.save(\"../Dataset/q2_train_final.npy\", q2_train_final)\n",
    "np.save(\"../Dataset/q1_valid_final.npy\", q1_valid_final)\n",
    "np.save(\"../Dataset/q2_valid_final.npy\", q2_valid_final)\n",
    "np.save(\"../Dataset/train_manual_features_final.npy\", train_manual_features_final)\n",
    "np.save(\"../Dataset/valid_manual_features_final.npy\", valid_manual_features_final)\n",
    "np.save(\"../Dataset/y_train_final.npy\", y_train_final)\n",
    "np.save(\"../Dataset/y_valid_final.npy\", y_valid_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../Dataset/MyVersion/q1_train_final.npy\", q1_train_final)\n",
    "np.save(\"../Dataset/MyVersion/q2_train_final.npy\", q2_train_final)\n",
    "np.save(\"../Dataset/MyVersion/q1_valid_final.npy\", q1_valid_final)\n",
    "np.save(\"../Dataset/MyVersion/q2_valid_final.npy\", q2_valid_final)\n",
    "np.save(\"../Dataset/MyVersion/train_manual_features_final.npy\", train_manual_features_final)\n",
    "np.save(\"../Dataset/MyVersion/valid_manual_features_final.npy\", valid_manual_features_final)\n",
    "np.save(\"../Dataset/MyVersion/y_train_final.npy\", y_train_final)\n",
    "np.save(\"../Dataset/MyVersion/y_valid_final.npy\", y_valid_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_train_final = np.load(\"../Dataset/q1_train_final.npy\", allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
